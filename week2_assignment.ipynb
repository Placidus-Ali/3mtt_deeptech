{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30d4b2-d4d7-4417-98fc-3fa5149fa3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea7dca1-8550-4441-a5aa-52856132d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9725a3bb-62c7-45c4-b13e-41673e953066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "IMG_SIZE = (128, 128) ## to resize all images to 128x128 pixels\n",
    "BATCH_SIZE = 32 ## process 32 images at a time during training\n",
    "EPOCHS = 2 ## train the model for 2 full passes over the dataset ie showing the each image twice to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cad571-795f-4d61-a374-798276dc676b",
   "metadata": {},
   "source": [
    "Here we resized the image to 128x128 pixels for consistency. The model will process the data in small groups (batches) of 32 images to efficiently use memory and improve training speed. The training will run for 2 epochs, meaning the entire dataset will be used twice during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02fbb03-771a-47f1-90ca-6f2c7824cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4774 images belonging to 24 classes.\n",
      "Found 4774 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Image Data (creates an image generator that normalizes pixel values and splits data for training and validation)\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, ## This divides every pixel value in the image by 255\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    \"boat_data/boat_data/train\",       # make sure this folder contains gondola, motorboat, ferry\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    #subset='training'\n",
    ") ## The above loads and preprocesses training images from the boat_data folder\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    \"boat_data/boat_data/test\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    #subset='validation'\n",
    ") ## While this one loads and preprocesses validation images from the same folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca907bc-e543-45aa-a85a-a593d814f5b9",
   "metadata": {},
   "source": [
    "Here we normalized the image pixel values to a 0–1 range and split the dataset into 80% training and 20% validation. Images were loaded from the boat_data folder, resized to 128x128 pixels, and grouped into batches of 32. The data generator automatically assigned labels based on folder names (e.g., gondola, motorboat, ferry) using categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85ac0a57-fe8b-42bc-b35d-cfc990103175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    MaxPooling2D(2, 2), ## specifies that the activation function used within a layer is the Rectified Linear Unit (ReLU). \n",
    "                        ## This means that the neuron will output the input directly if it's positive, and zero if it's negative\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'), \n",
    "    Dropout(0.5), \n",
    "    Dense(train_gen.num_classes, activation='softmax')  \n",
    "]) ## # Output layer 24 for all the 24 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11d7f7-bb33-482b-a990-700023fc8b0f",
   "metadata": {},
   "source": [
    "Here we built a Convolutional Neural Network (CNN) to classify boat images their categories. The model includes convolutional and pooling layers to extract image features, followed by flattening and dense layers to make predictions. A dropout layer was added to reduce overfitting, and the final softmax layer outputs probabilities for each of the three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5571e5e-59db-49ed-8fd3-f64f3e0c02af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57600</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,372,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,096</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57600\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m7,372,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │         \u001b[38;5;34m3,096\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,395,416</span> (28.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,395,416\u001b[0m (28.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,395,416</span> (28.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,395,416\u001b[0m (28.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) ## Prepares the model for training using the Adam optimizer, categorical crossentropy loss \n",
    "                                    ## (for multi-class classification), and accuracy as the evaluation metric\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8acaa8-0a52-4850-af94-e277bb7433ae",
   "metadata": {},
   "source": [
    "Here we compiled the CNN model using the Adam optimizer for adaptive learning, with categorical crossentropy as the loss function suitable for multi-class classification. Accuracy was chosen to track model performance during training. The model.summary() command gives a quick overview of the model’s structure and total trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a29b622d-02d7-4bc1-b4e5-6bb2b254ad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.4071 - loss: 2.2922 - val_accuracy: 0.6267 - val_loss: 1.2881\n",
      "Epoch 2/2\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 4s/step - accuracy: 0.6087 - loss: 1.4028 - val_accuracy: 0.7392 - val_loss: 0.9277\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51918dda-8ff3-4bda-b891-962a9e28df20",
   "metadata": {},
   "source": [
    "Here we trained the CNN model using the prepared training data and validated its performance with the validation set. The model learned over 2 epochs, adjusting its internal parameters to reduce classification errors. Training progress, including accuracy and loss for both training and validation, was stored in the history variable for future analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "82cf3456-f320-4959-a66e-e90123de1c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted class: Lanciafino10mBianca \n"
     ]
    }
   ],
   "source": [
    "# Path to the image you want to predict\n",
    "img_path = \"boat_data/image1.jpg\"  # Replace with your image path\n",
    "\n",
    "# Preprocess the image\n",
    "img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array /= 255.0  # Rescale pixel values to [0, 1]\n",
    "\n",
    "# Make prediction\n",
    "pred = model.predict(img_array)\n",
    "\n",
    "# Get class labels (based on the train_gen class indices)\n",
    "class_labels = {v: k for k, v in train_gen.class_indices.items()}  # Inverse mapping\n",
    "\n",
    "# Print predicted class\n",
    "predicted_class = class_labels[np.argmax(pred)]  # Get class with the highest probability\n",
    "print(f\"Predicted class: {predicted_class} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aabb51-3bbb-41dd-9314-9083b9349ae9",
   "metadata": {},
   "source": [
    "Here we loaded and preprocessed an image for prediction. The image was resized, converted to an array, and normalized. Finally, the model then predicted the class as **Lanciafino10mBianca**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e04ddac1-98c4-4547-903e-bc50f86a2681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step\n",
      "Predicted class: Patanella \n"
     ]
    }
   ],
   "source": [
    "# Path to the image you want to predict\n",
    "img_path = \"boat_data/image.jpg\"  # Replace with your image path\n",
    "\n",
    "# Preprocess the image\n",
    "img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array /= 255.0  # Rescale pixel values to [0, 1]\n",
    "\n",
    "# Make prediction\n",
    "pred = model.predict(img_array)\n",
    "\n",
    "# Get class labels (based on the train_gen class indices)\n",
    "class_labels = {v: k for k, v in train_gen.class_indices.items()}  # Inverse mapping\n",
    "\n",
    "# Print predicted class\n",
    "predicted_class = class_labels[np.argmax(pred)]  # Get class with the highest probability\n",
    "print(f\"Predicted class: {predicted_class} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d60c5-d838-4d62-8f8c-8a09a1254906",
   "metadata": {},
   "source": [
    "We tried another image and it predicted it to be **Patanella**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d9a4c-ac9e-4eff-80c7-70e5c39722cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
